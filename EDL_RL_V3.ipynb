{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.ndimage as nd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.functional import one_hot, log_softmax, softmax, normalize\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download mnist data\n",
    "data_train = MNIST(\"./data/mnist\",\n",
    "                   download=True,\n",
    "                   train=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "data_val = MNIST(\"./data/mnist\",\n",
    "                 train=False,\n",
    "                 download=True,\n",
    "                 transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    data_train, batch_size=1000, shuffle=True, num_workers=8)\n",
    "dataloader_val = DataLoader(data_val, batch_size=1000, num_workers=8)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": dataloader_train,\n",
    "    \"val\": dataloader_val,\n",
    "}\n",
    "\n",
    "digit_one, _ = data_val[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAMmElEQVR4nO3df6zddX3H8deLeqmhYNIOW2upgqxGiMmKue02cQuOiNDNtBp19g/WJWQ1GWSSmEzCktlkS1aXqdHoSC7QWDeFmCihMUztGjbCH+s4ZaW0dlLETi5tekGmFJXbe2/f++N+WS7lnu+5/X6/53xP+34+kptzzvf9/fHOSV/9fs/5fO/9OCIE4Px3QdsNABgMwg4kQdiBJAg7kARhB5J4wyAPdqEXxxu1ZJCHBFJ5Rb/UqZj0fLVaYbd9o6QvSVok6Z6I2F62/hu1RL/t6+scEkCJvbGna63yZbztRZK+KukmSVdL2mz76qr7A9BfdT6zr5f0dEQ8ExGnJN0vaWMzbQFoWp2wr5L07JzX48Wy17C91XbHdmdKkzUOB6COOmGf70uA1917GxFjETEaEaMjWlzjcADqqBP2cUmr57y+TNKxeu0A6Jc6YX9M0hrbV9i+UNInJO1qpi0ATas89BYR07Zvk/R9zQ697YiIQ411BqBRtcbZI+IhSQ811AuAPuJ2WSAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5KoNYsr0MvP/+R3u9b2br+rdNurv/rnpfW3fe4/S+sxPV1az6ZW2G0flXRS0oyk6YgYbaIpAM1r4sz+/oh4oYH9AOgjPrMDSdQNe0j6ge19trfOt4LtrbY7tjtTmqx5OABV1b2MvzYijtleLmm37f+OiEfmrhARY5LGJOlNXhY1jwegolpn9og4VjxOSHpA0vommgLQvMpht73E9iWvPpd0g6SDTTUGoFl1LuNXSHrA9qv7+WZEfK+RrnDOeMOqt5bW/+av76m87x/e+o+l9Zu+/Hul9Th5svKxz0eVwx4Rz0j6rQZ7AdBHDL0BSRB2IAnCDiRB2IEkCDuQBL/iilomPvj20voNF01V3vd7On9cWn/zy09V3ndGnNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2VHqgosuKq1/8C8e7duxF9+/tHyF4A8fnQ3O7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOPsKDX53qtK63+7/N7K+/7V6VOl9Td98z8q7xuvx5kdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnB2lfvKRRX3b90ePbOqxxrG+HTujnmd22ztsT9g+OGfZMtu7bR8pHnv8lQEAbVvIZfzXJN14xrI7JO2JiDWS9hSvAQyxnmGPiEckvXjG4o2SdhbPd0ra1GxbAJpW9Qu6FRFxXJKKx+XdVrS91XbHdmdKkxUPB6Cuvn8bHxFjETEaEaMjWtzvwwHoomrYT9heKUnF40RzLQHoh6ph3yVpS/F8i6QHm2kHQL/0HGe3fZ+k6yRdantc0mclbZf0Ldu3SPqppI/1s0m05w/XPVFr+1+c/nXX2tS2FaXbXsA4e6N6hj0iNncpXd9wLwD6iNtlgSQIO5AEYQeSIOxAEoQdSIJfcU1ucsO60vpXVt1da//j091rF/z7f9XaN84OZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9uROrBvp6/4/9N3bu9bWaG9fj43X4swOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzp7chdf8b63tD5/6VWn9XV9+oWttptaRcbY4swNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoyzn+de+aP1pfXOurt67GFRafVHU8tL6zNP/bjH/jEoPc/stnfYnrB9cM6ybbafs72/+NnQ3zYB1LWQy/ivSbpxnuVfjIi1xc9DzbYFoGk9wx4Rj0h6cQC9AOijOl/Q3Wb7QHGZv7TbSra32u7Y7kxpssbhANRRNex3SbpS0lpJxyV9vtuKETEWEaMRMTqixRUPB6CuSmGPiBMRMRMRpyXdLan8K18ArasUdtsr57z8sKSD3dYFMBx6jrPbvk/SdZIutT0u6bOSrrO9VlJIOirpk/1rEXX8+tLycfIRl9d7+ct9HymtX6EDtfaP5vQMe0RsnmfxvX3oBUAfcbsskARhB5Ig7EAShB1IgrADSfArrue5yU0/r7V9rz8Vfdk9/Z3yGc3hzA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDOfh5Y9M4ru9Y66/6519al1X95+d2l9ZF/3ddj/xgWnNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2c8DJ97ffdrkun8q+isPf6C0vkZ7a+0fg8OZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJz9PPDKMlfedt/kqdL6VZ8bL61PVz4yBq3nmd32atsP2z5s+5DtTxXLl9nebftI8bi0/+0CqGohl/HTkj4dEVdJ+h1Jt9q+WtIdkvZExBpJe4rXAIZUz7BHxPGIeLx4flLSYUmrJG2UtLNYbaekTX3qEUADzuoLOtuXS7pG0l5JKyLiuDT7H4KkeW/Qtr3Vdsd2Z0qTNdsFUNWCw277YknflnR7RLy00O0iYiwiRiNidESLq/QIoAELCrvtEc0G/RsR8Z1i8QnbK4v6SkkT/WkRQBN6Dr3ZtqR7JR2OiC/MKe2StEXS9uLxwb50iJ6W/8Fzlbfd9dI1pfWZ51+ovG8Ml4WMs18r6WZJT9reXyy7U7Mh/5btWyT9VNLH+tIhgEb0DHtEPCqp210b1zfbDoB+4XZZIAnCDiRB2IEkCDuQBGEHkuBXXM8BXlx+5+HGtz5Red8/O3VxaT0mucX5fMGZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJz9XDAzU1oeO/y+rrXb33u0dNt/e/Y3S+urdKi0jnMHZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9nNATJdPjHz5Hb/sWrvq724u3db7L6nUE849nNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IImFzM++WtLXJb1F0mlJYxHxJdvbJP2ZpOeLVe+MiIf61Si6m3n6J11rb2MibRQWclPNtKRPR8Tjti+RtM/27qL2xYj4h/61B6ApC5mf/bik48Xzk7YPS1rV78YANOusPrPbvlzSNZL2Fotus33A9g7bS7tss9V2x3ZnSkwlBLRlwWG3fbGkb0u6PSJeknSXpCslrdXsmf/z820XEWMRMRoRoyMqn7MMQP8sKOy2RzQb9G9ExHckKSJORMRMRJyWdLek9f1rE0BdPcNu25LulXQ4Ir4wZ/nKOat9WNLB5tsD0JSFfBt/raSbJT1pe3+x7E5Jm22vlRSSjkr6ZB/6A9CQhXwb/6gkz1NiTB04h3AHHZAEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAlHxOAOZj8v6X/mLLpU0gsDa+DsDGtvw9qXRG9VNdnb2yPizfMVBhr21x3c7kTEaGsNlBjW3oa1L4neqhpUb1zGA0kQdiCJtsM+1vLxywxrb8Pal0RvVQ2kt1Y/swMYnLbP7AAGhLADSbQSdts32v6R7adt39FGD93YPmr7Sdv7bXda7mWH7QnbB+csW2Z7t+0jxeO8c+y11Ns2288V791+2xta6m217YdtH7Z9yPaniuWtvnclfQ3kfRv4Z3bbiyQ9JekDksYlPSZpc0T8cKCNdGH7qKTRiGj9Bgzbvy/pZUlfj4h3F8v+XtKLEbG9+I9yaUR8Zkh62ybp5ban8S5mK1o5d5pxSZsk/alafO9K+vq4BvC+tXFmXy/p6Yh4JiJOSbpf0sYW+hh6EfGIpBfPWLxR0s7i+U7N/mMZuC69DYWIOB4RjxfPT0p6dZrxVt+7kr4Goo2wr5L07JzX4xqu+d5D0g9s77O9te1m5rEiIo5Ls/94JC1vuZ8z9ZzGe5DOmGZ8aN67KtOf19VG2OebSmqYxv+ujYj3SLpJ0q3F5SoWZkHTeA/KPNOMD4Wq05/X1UbYxyWtnvP6MknHWuhjXhFxrHickPSAhm8q6hOvzqBbPE603M//G6ZpvOebZlxD8N61Of15G2F/TNIa21fYvlDSJyTtaqGP17G9pPjiRLaXSLpBwzcV9S5JW4rnWyQ92GIvrzEs03h3m2ZcLb93rU9/HhED/5G0QbPfyP9Y0l+10UOXvt4h6Yni51DbvUm6T7OXdVOavSK6RdJvSNoj6UjxuGyIevsnSU9KOqDZYK1sqbf3afaj4QFJ+4ufDW2/dyV9DeR943ZZIAnuoAOSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJP4Pye69BOITPJMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Single digit test \n",
    "plt.imshow(digit_one.reshape(28,28))\n",
    "plt.show()\n",
    "\n",
    "def rotate_img(x, deg):\n",
    "    rot_img = scipy.ndimage.rotate(x,deg,reshape=False)\n",
    "    return rot_img\n",
    "\n",
    "#Example rotation of digit one by 90 degrees\n",
    "rot_one = rotate_img(digit_one.reshape(28,28),90)\n",
    "\n",
    "tens_rot_one = rot_one\n",
    "plt.imshow(tens_rot_one)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Setup/utility functions\n",
    "def get_device():\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    return device\n",
    "\n",
    "def one_hot_embedding(labels, num_classes=10):\n",
    "    # Convert to One Hot Encoding\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]\n",
    "\n",
    "#Check GPU available\n",
    "device = get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Functions\n",
    "def relu_evidence(y):\n",
    "    return F.relu(y)\n",
    "\n",
    "def kl_divergence(alpha, num_classes, device=None):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    ones = torch.ones([1, num_classes], dtype=torch.float32, device=device)\n",
    "    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)\n",
    "    first_term = (\n",
    "        torch.lgamma(sum_alpha)\n",
    "        - torch.lgamma(alpha).sum(dim=1, keepdim=True)\n",
    "        + torch.lgamma(ones).sum(dim=1, keepdim=True)\n",
    "        - torch.lgamma(ones.sum(dim=1, keepdim=True))\n",
    "    )\n",
    "    second_term = (\n",
    "        (alpha - ones)\n",
    "        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))\n",
    "        .sum(dim=1, keepdim=True)\n",
    "    )\n",
    "    kl = first_term + second_term\n",
    "    return kl\n",
    "\n",
    "def edl_loss(func, y, alpha, epoch_num, num_classes, annealing_step, device=None):\n",
    "    y = y.to(device)\n",
    "    alpha = alpha.to(device)\n",
    "    S = torch.sum(alpha, dim=1, keepdim=True)\n",
    "\n",
    "    A = torch.sum(y * (func(S) - func(alpha)), dim=1, keepdim=True)\n",
    "\n",
    "    annealing_coef = torch.min(\n",
    "        torch.tensor(1.0, dtype=torch.float32),\n",
    "        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),\n",
    "    )\n",
    "\n",
    "    kl_alpha = (alpha - 1) * (1 - y) + 1\n",
    "    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)\n",
    "    return A + kl_div\n",
    "\n",
    "def edl_digamma_loss(\n",
    "    output, target, epoch_num, num_classes, annealing_step, device=None\n",
    "):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    evidence = relu_evidence(output)\n",
    "    alpha = evidence + 1\n",
    "    loss = torch.mean(\n",
    "        edl_loss(\n",
    "            torch.digamma, target, alpha, epoch_num, num_classes, annealing_step, device\n",
    "        )\n",
    "    )\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2018 EDL Implementation\n",
    "class InferNet(nn.Module):\n",
    "    def __init__(self, sample_shape, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "        if len(sample_shape) == 1:\n",
    "            self.conv = nn.Sequential()\n",
    "            fc_in = sample_shape[0]\n",
    "        else:  # 3\n",
    "            dims = [sample_shape[0], 20, 50]\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=dims[0], out_channels=dims[1], kernel_size=5, stride=1, padding=2),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=dims[1], out_channels=dims[2], kernel_size=5, stride=1, padding=2),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "            fc_in = sample_shape[1] // 4 * sample_shape[2] // 4 * dims[2]\n",
    "\n",
    "        fc_dims = [fc_in, min(fc_in, 500), num_classes]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(fc_dims[0], fc_dims[1]),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_dims[1], fc_dims[2]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_conv = self.conv(x).view(x.shape[0], -1)\n",
    "        evidence = self.fc(out_conv)\n",
    "        return evidence\n",
    "\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    dataloaders,\n",
    "    num_classes,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler=None,\n",
    "    num_epochs=30,\n",
    "    device=None,\n",
    "    uncertainty=False,\n",
    "):\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    losses = {\"loss\": [], \"phase\": [], \"epoch\": []}\n",
    "    accuracy = {\"accuracy\": [], \"phase\": [], \"epoch\": []}\n",
    "    evidences = {\"evidence\": [], \"type\": [], \"epoch\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"val\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            correct = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for i, (inputs, labels) in enumerate(dataloaders[phase]):\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "\n",
    "                    if uncertainty:\n",
    "                        y = one_hot_embedding(labels, num_classes)\n",
    "                        y = y.to(device)\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(\n",
    "                            outputs, y.float(), epoch, num_classes, 10, device\n",
    "                        )\n",
    "\n",
    "                        match = torch.reshape(torch.eq(preds, labels).float(), (-1, 1))\n",
    "                        acc = torch.mean(match)\n",
    "                        evidence = relu_evidence(outputs)\n",
    "                        alpha = evidence + 1\n",
    "                        u = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "\n",
    "                        total_evidence = torch.sum(evidence, 1, keepdim=True)\n",
    "                        mean_evidence = torch.mean(total_evidence)\n",
    "                        mean_evidence_succ = torch.sum(\n",
    "                            torch.sum(evidence, 1, keepdim=True) * match\n",
    "                        ) / torch.sum(match + 1e-20)\n",
    "                        mean_evidence_fail = torch.sum(\n",
    "                            torch.sum(evidence, 1, keepdim=True) * (1 - match)\n",
    "                        ) / (torch.sum(torch.abs(1 - match)) + 1e-20)\n",
    "\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if scheduler is not None:\n",
    "                if phase == \"train\":\n",
    "                    scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            losses[\"loss\"].append(epoch_loss)\n",
    "            losses[\"phase\"].append(phase)\n",
    "            losses[\"epoch\"].append(epoch)\n",
    "            accuracy[\"accuracy\"].append(epoch_acc.item())\n",
    "            accuracy[\"epoch\"].append(epoch)\n",
    "            accuracy[\"phase\"].append(phase)\n",
    "\n",
    "            print(\n",
    "                \"{} loss: {:.4f} acc: {:.4f}\".format(\n",
    "                    phase.capitalize(), epoch_loss, epoch_acc\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == \"val\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())                \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(\n",
    "        \"Training complete in {:.0f}m {:.0f}s\".format(\n",
    "            time_elapsed // 60, time_elapsed % 60\n",
    "        )\n",
    "    )\n",
    "    print(\"Best val Acc: {:4f}\".format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), 'mnist_EDL_model.pyt')\n",
    "\n",
    "    metrics = (losses, accuracy)\n",
    "\n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/49\n",
      "----------\n",
      "Train loss: 1.3088 acc: 0.6919\n",
      "Val loss: 0.6301 acc: 0.8988\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "Train loss: 0.7573 acc: 0.9052\n",
      "Val loss: 0.5337 acc: 0.9486\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "Train loss: 0.6159 acc: 0.9406\n",
      "Val loss: 0.4723 acc: 0.9641\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "Train loss: 0.5561 acc: 0.9541\n",
      "Val loss: 0.4599 acc: 0.9656\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "Train loss: 0.5380 acc: 0.9572\n",
      "Val loss: 0.4323 acc: 0.9719\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "Train loss: 0.5260 acc: 0.9586\n",
      "Val loss: 0.4323 acc: 0.9725\n",
      "\n",
      "Epoch 6/49\n",
      "----------\n",
      "Train loss: 0.5182 acc: 0.9604\n",
      "Val loss: 0.4368 acc: 0.9714\n",
      "\n",
      "Epoch 7/49\n",
      "----------\n",
      "Train loss: 0.4800 acc: 0.9668\n",
      "Val loss: 0.4044 acc: 0.9757\n",
      "\n",
      "Epoch 8/49\n",
      "----------\n",
      "Train loss: 0.4692 acc: 0.9679\n",
      "Val loss: 0.4087 acc: 0.9749\n",
      "\n",
      "Epoch 9/49\n",
      "----------\n",
      "Train loss: 0.4767 acc: 0.9680\n",
      "Val loss: 0.4127 acc: 0.9754\n",
      "\n",
      "Epoch 10/49\n",
      "----------\n",
      "Train loss: 0.4786 acc: 0.9677\n",
      "Val loss: 0.4140 acc: 0.9757\n",
      "\n",
      "Epoch 11/49\n",
      "----------\n",
      "Train loss: 0.4777 acc: 0.9688\n",
      "Val loss: 0.4163 acc: 0.9763\n",
      "\n",
      "Epoch 12/49\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "#Train Model or Load Pre-Trained\n",
    "model = None\n",
    "train = True \n",
    "if(train):\n",
    "    #Initalise and train Model & Validate\n",
    "\n",
    "    num_epochs = 50\n",
    "    use_uncertainty = True\n",
    "    num_classes = 10\n",
    "\n",
    "    #model = LeNet(num_classes, digit_one.shape,dropout=True) \n",
    "\n",
    "    model = InferNet(digit_one.shape,num_classes)\n",
    "    if use_uncertainty:\n",
    "        criterion = edl_digamma_loss\n",
    "        #criterion = edl_loss\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.005)\n",
    "\n",
    "    exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "\n",
    "    model, metrics = train_model(\n",
    "        model,\n",
    "        dataloaders,\n",
    "        num_classes,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler=exp_lr_scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device,\n",
    "        uncertainty=use_uncertainty,\n",
    "    )\n",
    "\n",
    "    model_state = {\n",
    "        \"epoch\": num_epochs,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }\n",
    "\n",
    "else:\n",
    "    #Load Pre-Trained Model \n",
    "    num_classes = 10\n",
    "    #model = LeNet(num_classes, digit_one.shape,dropout=True) \n",
    "    model = InferNet(digit_one.shape,num_classes)\n",
    "\n",
    "    model.load_state_dict(torch.load('mnist_EDL_model.pyt'))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do things with trained model\n",
    "def model_predict(model, input):\n",
    "    input = input.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        model = model.to(device)\n",
    "        output = model(input)\n",
    "        evidence = relu_evidence(output)\n",
    "        alpha = evidence + 1\n",
    "        uncertainty = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        prob = alpha / torch.sum(alpha, dim=1, keepdim=True)\n",
    "    return prob, preds, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_uncertainty = True\n",
    "def rotating_image_classification(\n",
    "    model, img, uncertainty=True, threshold=0.5, device=None):\n",
    "    if not device:\n",
    "        device = get_device()\n",
    "    num_classes = 10\n",
    "    Mdeg = 360\n",
    "    Ndeg = int(Mdeg / 10) + 1\n",
    "    ldeg = []\n",
    "    lp = []\n",
    "    lu = []\n",
    "    classifications = []\n",
    "\n",
    "    scores = np.zeros((1, num_classes))\n",
    "    rimgs = np.zeros((28, 28 * Ndeg))\n",
    "    for i, deg in enumerate(np.linspace(0, Mdeg, Ndeg)):\n",
    "        nimg = rotate_img(img.numpy()[0], deg).reshape(28, 28)\n",
    "        nimg = np.clip(a=nimg, a_min=0, a_max=1)\n",
    "        rimgs[:, i * 28 : (i + 1) * 28] = nimg\n",
    "        trans = transforms.ToTensor()\n",
    "        img_tensor = trans(nimg)\n",
    "        img_tensor.unsqueeze_(0)\n",
    "        img_variable = torch.autograd.Variable(img_tensor)\n",
    "        img_variable = img_variable.to(device)\n",
    "\n",
    "        if uncertainty:\n",
    "            output = model(img_variable)\n",
    "            evidence = relu_evidence(output)\n",
    "            alpha = evidence + 1\n",
    "            uncertainty = num_classes / torch.sum(alpha, dim=1, keepdim=True)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            prob = alpha / torch.sum(alpha, dim=1, keepdim=True)\n",
    "            output = output.flatten()\n",
    "            prob = prob.flatten()\n",
    "            preds = preds.flatten()\n",
    "            classifications.append(preds[0].item())\n",
    "            lu.append(uncertainty.mean().tolist())\n",
    "        else:\n",
    "            output = model(img_variable)\n",
    "            _, preds, = torch.max(output, 1)\n",
    "            prob = F.softmax(output, dim=1)\n",
    "            output = output.flatten()\n",
    "            prob = prob.flatten()\n",
    "            preds = preds.flatten()\n",
    "            classifications.append(preds[0].item())\n",
    "\n",
    "        scores += prob.detach().cpu().numpy() >= threshold\n",
    "        ldeg.append(deg)\n",
    "        lp.append(prob.tolist())\n",
    "\n",
    "    labels = np.arange(10)[scores[0].astype(bool)]\n",
    "    lp = np.array(lp)[:, labels]\n",
    "    c = [\"black\", \"blue\", \"red\", \"brown\", \"purple\", \"cyan\"]\n",
    "    marker = [\"s\", \"^\", \"o\"] * 2\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        plt.plot(ldeg, lp[:, i], marker=marker[i], c=c[i])\n",
    "\n",
    "    if uncertainty:\n",
    "        labels += [\"uncertainty\"]\n",
    "        plt.plot(ldeg, lu, marker=\"<\", c=\"red\")\n",
    "\n",
    "    plt.legend(labels)\n",
    " \n",
    "    plt.xlim([0,Mdeg])  \n",
    "    plt.xlabel('Rotation Degree')\n",
    "    plt.ylabel('Classification Probability')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=[6.2,100])\n",
    "    plt.imshow(1-rimgs,cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "rotating_image_classification(\n",
    "    model, digit_one, uncertainty=use_uncertainty\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(states, reward, action_dist, n_ep,n_epoch):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    total_ep_reward = np.round(np.sum(reward))\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.title('Epoch {}: Reward Values for Episode {}, Total Reward= {}'.format(n_epoch,n_ep,total_ep_reward))\n",
    "    plt.xlabel('Timestemp')\n",
    "    plt.ylabel('Reward Value')\n",
    "    plt.plot(reward)\n",
    "\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    #rotations_t = torch.tensor(states, dtype=torch.float)\n",
    "    plt.title('Training States for Episode '+str(n_ep))\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('Rotation Degree')\n",
    "    plt.plot(states)\n",
    "    plt.ylim([0, 360])\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.title('Action Probability Distribution for Episode '+str(n_ep))\n",
    "    names = ['Increase', 'Decrease', 'Maintain']\n",
    "    plt.bar(names,action_dist)\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython and False:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table is replaced by a neural network\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, observation_space_size: int, action_space_size: int, hidden_size: int):\n",
    "        super(Agent, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=observation_space_size, out_features=hidden_size, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=hidden_size, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=action_space_size, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = normalize(x, dim=1)\n",
    "        x = F.softmax(self.net(x),dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "episode_len = 720\n",
    "class mnist_rotate_env():\n",
    "    def __init__(self,input_digit):\n",
    "        self.state = None\n",
    "        self.digit = input_digit\n",
    "        self.timestep = 0\n",
    "\n",
    "    def step(self,action):\n",
    "        assert self.state is not None, \"Call reset before using step method.\"\n",
    "        assert action in [0,1,2]\n",
    "        assert self.digit is not None\n",
    "\n",
    "        #Apply action to find new state\n",
    "        new_state = self.state\n",
    "    \n",
    "        if(action==0): \n",
    "            new_state[1] = new_state[1] + 5\n",
    "            if(new_state[1]>359):\n",
    "                new_state[1] = new_state[1]-360 \n",
    "        elif(action==1):\n",
    "            new_state[1] = new_state[1] - 5\n",
    "            if(new_state[1]<0):\n",
    "                new_state[1] = 360+new_state[1]\n",
    "\n",
    "        new_rotated_one = rotate_img(self.digit.reshape(28,28),new_state[1])\n",
    "        new_rotated_one = torch.tensor(new_rotated_one[None, :])\n",
    "        prob, preds,uncertainty = model_predict(model,new_rotated_one)\n",
    "\n",
    "        #Calculate reward\n",
    "        reward = (1-uncertainty.item()) \n",
    "\n",
    "        self.state = new_state #Set new state\n",
    "        self.timestep = self.timestep + 1 #Increment time\n",
    "\n",
    "        done = (self.timestep==episode_len)\n",
    "\n",
    "        return new_state, reward, done #preds.item()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        spawn_rotation = random.randint(0,360)\n",
    "        current_rotation = spawn_rotation\n",
    "        self.state = [spawn_rotation,current_rotation]\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params:\n",
    "    NUM_EPOCHS = 5000\n",
    "    ALPHA = 5e-3        # learning rate, default = 5e-3\n",
    "    BATCH_SIZE = 30    # how many episodes we want to pack into an epoch, default = 64\n",
    "    GAMMA = 0.99        # discount rate\n",
    "    HIDDEN_SIZE = 64    # number of hidden nodes we have in our dnn\n",
    "    BETA = 0.1          # the entropy bonus multiplier\n",
    "\n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(self, input = digit_one, use_cuda: bool = True):\n",
    "\n",
    "        self.input = input\n",
    "        self.NUM_EPOCHS = Params.NUM_EPOCHS\n",
    "        self.ALPHA = Params.ALPHA\n",
    "        self.BATCH_SIZE = Params.BATCH_SIZE\n",
    "        self.GAMMA = Params.GAMMA\n",
    "        self.HIDDEN_SIZE = Params.HIDDEN_SIZE\n",
    "        self.BETA = Params.BETA\n",
    "        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() and use_cuda else 'cpu')\n",
    "\n",
    "        # instantiate the tensorboard writer\n",
    "        self.writer = SummaryWriter(comment=f'_PG_CP_Gamma={self.GAMMA},'\n",
    "                                            f'LR={self.ALPHA},'\n",
    "                                            f'BS={self.BATCH_SIZE},'\n",
    "                                            f'NH={self.HIDDEN_SIZE},'\n",
    "                                            f'BETA={self.BETA}')\n",
    "\n",
    "        # create the environment\n",
    "        #Environemnt Parameters\n",
    "        self.obs_state_size = 2\n",
    "        self.action_state_size = 3\n",
    "\n",
    "        self.env = mnist_rotate_env(input)\n",
    "\n",
    "        # the agent driven by a neural network architecture\n",
    "        self.agent = Agent(observation_space_size=self.obs_state_size,\n",
    "                           action_space_size=self.action_state_size,\n",
    "                           hidden_size=self.HIDDEN_SIZE).to(self.DEVICE)\n",
    "\n",
    "        self.adam = optim.Adam(params=self.agent.parameters(), lr=self.ALPHA)\n",
    "\n",
    "        self.total_rewards = deque([], maxlen=100)\n",
    "\n",
    "        # flag to figure out if we have render a single episode current epoch\n",
    "        self.finished_rendering_this_epoch = False\n",
    "\n",
    "    def solve_environment(self):\n",
    "        \"\"\"\n",
    "            The main interface for the Policy Gradient solver\n",
    "        \"\"\"\n",
    "        # init the episode and the epoch\n",
    "        episode = 0\n",
    "        epoch = 0\n",
    "\n",
    "        # init the epoch arrays\n",
    "        # used for entropy calculation\n",
    "        epoch_logits = torch.empty(size=(0, self.action_state_size), device=self.DEVICE)\n",
    "        epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=self.DEVICE)\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # play an episode of the environment\n",
    "            (episode_weighted_log_prob_trajectory,\n",
    "             episode_logits,\n",
    "             sum_of_episode_rewards,\n",
    "             episode) = self.play_episode(episode=episode,epoch=epoch)\n",
    "\n",
    "            # after each episode append the sum of total rewards to the deque\n",
    "            self.total_rewards.append(sum_of_episode_rewards)\n",
    "\n",
    "            # append the weighted log-probabilities of actions\n",
    "            epoch_weighted_log_probs = torch.cat((epoch_weighted_log_probs, episode_weighted_log_prob_trajectory),\n",
    "                                                 dim=0)\n",
    "\n",
    "            # append the logits - needed for the entropy bonus calculation\n",
    "            epoch_logits = torch.cat((epoch_logits, episode_logits), dim=0)\n",
    "\n",
    "            # if the epoch is over - we have epoch trajectories to perform the policy gradient\n",
    "            if episode >= self.BATCH_SIZE:\n",
    "\n",
    "                # reset the rendering flag\n",
    "                self.finished_rendering_this_epoch = False\n",
    "\n",
    "                # reset the episode count\n",
    "                episode = 0\n",
    "\n",
    "                # increment the epoch\n",
    "                epoch += 1\n",
    "\n",
    "                # calculate the loss\n",
    "                loss, entropy = self.calculate_loss(epoch_logits=epoch_logits,\n",
    "                                                    weighted_log_probs=epoch_weighted_log_probs)\n",
    "\n",
    "                # zero the gradient\n",
    "                self.adam.zero_grad()\n",
    "\n",
    "                # backprop\n",
    "                loss.backward()\n",
    "\n",
    "                # update the parameters\n",
    "                self.adam.step()\n",
    "\n",
    "                # feedback\n",
    "                print(\"\\r\", f\"Epoch: {epoch}, Avg Return per Epoch: {np.mean(self.total_rewards):.3f}\",\n",
    "                      end=\"\",\n",
    "                      flush=True)\n",
    "\n",
    "                self.writer.add_scalar(tag='Average Return over 100 episodes',\n",
    "                                       scalar_value=np.mean(self.total_rewards),\n",
    "                                       global_step=epoch)\n",
    "\n",
    "                self.writer.add_scalar(tag='Entropy',\n",
    "                                       scalar_value=entropy,\n",
    "                                       global_step=epoch)\n",
    "\n",
    "                # reset the epoch arrays\n",
    "                # used for entropy calculation\n",
    "                epoch_logits = torch.empty(size=(0, self.action_state_size), device=self.DEVICE)\n",
    "                epoch_weighted_log_probs = torch.empty(size=(0,), dtype=torch.float, device=self.DEVICE)\n",
    "\n",
    "                # check if solved\n",
    "                if np.mean(self.total_rewards) > 550:\n",
    "                    print('\\nSolved!')\n",
    "                    torch.save(self.agent.state_dict(), 'best_EDL_RL_agent_model.pyt')\n",
    "                    break\n",
    "\n",
    "        # close the environment\n",
    "        #self.env.close()\n",
    "\n",
    "        # close the writer\n",
    "        self.writer.close()\n",
    "\n",
    "    def play_episode(self, episode: int, epoch: int):\n",
    "        \"\"\"\n",
    "            Plays an episode of the environment.\n",
    "            episode: the episode counter\n",
    "            Returns:\n",
    "                sum_weighted_log_probs: the sum of the log-prob of an action multiplied by the reward-to-go from that state\n",
    "                episode_logits: the logits of every step of the episode - needed to compute entropy for entropy bonus\n",
    "                finished_rendering_this_epoch: pass-through rendering flag\n",
    "                sum_of_rewards: sum of the rewards for the episode - needed for the average over 200 episode statistic\n",
    "        \"\"\"\n",
    "        # reset the environment to a random initial state every epoch\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # initialize the episode arrays\n",
    "        episode_actions = torch.empty(size=(0,), dtype=torch.long, device=self.DEVICE)\n",
    "        episode_logits = torch.empty(size=(0, self.action_state_size), device=self.DEVICE)\n",
    "        average_rewards = np.empty(shape=(0,), dtype=float)\n",
    "        episode_rewards = np.empty(shape=(0,), dtype=float)\n",
    "\n",
    "        #initalise metrics arrays\n",
    "        eps_states = []\n",
    "        eps_rewards = []\n",
    "        eps_action_dist = np.array([0,0,0])\n",
    "\n",
    "\n",
    "        # episode loop\n",
    "        while True:\n",
    "            # get the action logits from the agent - (preferences)\n",
    "            action_logits = self.agent(torch.tensor(state).float().unsqueeze(dim=0).to(self.DEVICE))\n",
    "\n",
    "            # append the logits to the episode logits list\n",
    "            episode_logits = torch.cat((episode_logits, action_logits), dim=0)\n",
    "\n",
    "            # sample an action according to the action distribution\n",
    "            action = Categorical(logits=action_logits).sample()\n",
    "\n",
    "            # append the action to the episode action list to obtain the trajectory\n",
    "            # we need to store the actions and logits so we could calculate the gradient of the performance\n",
    "            episode_actions = torch.cat((episode_actions, action), dim=0)\n",
    "\n",
    "            # take the chosen action, observe the reward and the next state\n",
    "            state, reward, done  = self.env.step(action=action.cpu().item())\n",
    "\n",
    "            #Update episode metrics\n",
    "            eps_states.append(state[1])\n",
    "            eps_rewards.append(reward)\n",
    "            eps_action_dist = eps_action_dist + action_logits.cpu().detach()[0].numpy()\n",
    "\n",
    "\n",
    "            # append the reward to the rewards pool that we collect during the episode\n",
    "            # we need the rewards so we can calculate the weights for the policy gradient\n",
    "            # and the baseline of average\n",
    "            episode_rewards = np.concatenate((episode_rewards, np.array([reward])), axis=0)\n",
    "\n",
    "            # here the average reward is state specific\n",
    "            average_rewards = np.concatenate((average_rewards,\n",
    "                                              np.expand_dims(np.mean(episode_rewards), axis=0)),\n",
    "                                             axis=0)\n",
    "\n",
    "            # the episode is over\n",
    "            if done:\n",
    "                # increment the episode\n",
    "                episode += 1\n",
    "\n",
    "                # turn the rewards we accumulated during the episode into the rewards-to-go:\n",
    "                # earlier actions are responsible for more rewards than the later taken actions\n",
    "                discounted_rewards_to_go = PolicyGradient.get_discounted_rewards(rewards=episode_rewards,\n",
    "                                                                                 gamma=self.GAMMA)\n",
    "                discounted_rewards_to_go -= average_rewards  # baseline - state specific average\n",
    "\n",
    "                # # calculate the sum of the rewards for the running average metric\n",
    "                sum_of_rewards = np.sum(episode_rewards)\n",
    "\n",
    "                # set the mask for the actions taken in the episode\n",
    "                mask = one_hot(episode_actions, num_classes=self.action_state_size)\n",
    "\n",
    "                # calculate the log-probabilities of the taken actions\n",
    "                # mask is needed to filter out log-probabilities of not related logits\n",
    "                episode_log_probs = torch.sum(mask.float() * log_softmax(episode_logits, dim=1), dim=1)\n",
    "\n",
    "                # weight the episode log-probabilities by the rewards-to-go\n",
    "                episode_weighted_log_probs = episode_log_probs * \\\n",
    "                    torch.tensor(discounted_rewards_to_go).float().to(self.DEVICE)\n",
    "\n",
    "                # calculate the sum over trajectory of the weighted log-probabilities\n",
    "                sum_weighted_log_probs = torch.sum(episode_weighted_log_probs).unsqueeze(dim=0)\n",
    "\n",
    "                # won't render again this epoch\n",
    "                self.finished_rendering_this_epoch = True\n",
    "\n",
    "                #Plot episode metrics\n",
    "                mean_action_dist = eps_action_dist/episode_len\n",
    "                plot_metrics(eps_states,eps_rewards, mean_action_dist, episode,epoch)\n",
    "\n",
    "                return sum_weighted_log_probs, episode_logits, sum_of_rewards, episode\n",
    "\n",
    "    def calculate_loss(self, epoch_logits: torch.Tensor, weighted_log_probs: torch.Tensor):\n",
    "        \"\"\"\n",
    "            Calculates the policy \"loss\" and the entropy bonus\n",
    "            Args:\n",
    "                epoch_logits: logits of the policy network we have collected over the epoch\n",
    "                weighted_log_probs: loP * W of the actions taken\n",
    "            Returns:\n",
    "                policy loss + the entropy bonus\n",
    "                entropy: needed for logging\n",
    "        \"\"\"\n",
    "        policy_loss = -1 * torch.mean(weighted_log_probs)\n",
    "\n",
    "        # add the entropy bonus\n",
    "        p = softmax(epoch_logits, dim=1)\n",
    "        log_p = log_softmax(epoch_logits, dim=1)\n",
    "        entropy = -1 * torch.mean(torch.sum(p * log_p, dim=1), dim=0)\n",
    "        entropy_bonus = -1 * self.BETA * entropy\n",
    "\n",
    "        return policy_loss + entropy_bonus, entropy\n",
    "\n",
    "    @staticmethod\n",
    "    def get_discounted_rewards(rewards: np.array, gamma: float) -> np.array:\n",
    "        \"\"\"\n",
    "            Calculates the sequence of discounted rewards-to-go.\n",
    "            Args:\n",
    "                rewards: the sequence of observed rewards\n",
    "                gamma: the discount factor\n",
    "            Returns:\n",
    "                discounted_rewards: the sequence of the rewards-to-go\n",
    "        \"\"\"\n",
    "        discounted_rewards = np.empty_like(rewards, dtype=float)\n",
    "        for i in range(rewards.shape[0]):\n",
    "            gammas = np.full(shape=(rewards[i:].shape[0]), fill_value=gamma)\n",
    "            discounted_gammas = np.power(gammas, np.arange(rewards[i:].shape[0]))\n",
    "            discounted_reward = np.sum(rewards[i:] * discounted_gammas)\n",
    "            discounted_rewards[i] = discounted_reward\n",
    "        return discounted_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reinforcement Learning Trial\n",
    "policy_gradient = PolicyGradient() \n",
    "policy_gradient.solve_environment()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e48212c6a16cad7ad725b1fc4aa1bf1bdbcdd62e8df189eb01c54244f280b21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
